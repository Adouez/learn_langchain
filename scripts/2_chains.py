from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# 1. Le ModÃ¨le
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# 2. Le Prompt Template (Le moule Ã  questions)
# On dÃ©finit un "rÃ´le" (System) et la question de l'utilisateur (Human)
prompt = ChatPromptTemplate.from_messages([
    ("system", "Tu es un expert technique en Python et LangChain. Tu expliques des concepts complexes avec des mÃ©taphores simples."),
    ("human", "{question}")
])

# 3. L'Output Parser (Le nettoyeur)
# Par dÃ©faut, le LLM renvoie un objet complexe (AIMessage). 
# Ce parser va extraire juste le texte (string) pour que ce soit plus propre.
output_parser = StrOutputParser()

# 4. La ChaÃ®ne (The Chain)
# C'est la magie de LangChain : on utilise le pipe "|" pour enchaÃ®ner les Ã©tapes.
# Prompt -> ModÃ¨le -> Nettoyeur
chain = prompt | llm | output_parser

# 5. ExÃ©cution
print("ğŸ¤– Le Professeur LangChain rÃ©flÃ©chis...")
reponse = chain.invoke({"question": "C'est quoi LangChain exactement ?"})

print("\nğŸ’¬ RÃ©ponse amÃ©liorÃ©e :")
print(reponse)
