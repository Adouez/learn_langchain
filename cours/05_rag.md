# üîé Le√ßon 5 : Le RAG (Retrieval Augmented Generation)

Le RAG est la technique qui permet de donner acc√®s √† vos propres donn√©es priv√©es (PDF, TXT, Bases de donn√©es) √† un LLM, sans avoir √† le r√©-entra√Æner.

## ‚öôÔ∏è Comment √ßa marche ? (Le Pipeline RAG)

C'est un processus en 3 √©tapes principales :

1.  **Ingestion & Indexation (Pr√©paration)**
    *   **Load** : On charge le fichier (ex: `TextLoader`, `PyPDFLoader`).
    *   **Split** : On d√©coupe le texte en petits morceaux ("chunks") pour ne pas d√©passer la limite de taille du mod√®le.
    *   **Embed** : On transforme ces morceaux en listes de nombres (vecteurs) gr√¢ce √† un mod√®le d'embedding (ex: `OpenAIEmbeddings`).
    *   **Store** : On sauvegarde ces vecteurs dans une base de donn√©es vectorielle (ex: `FAISS`, `ChromaDB`).

2.  **Retrieval (Recherche)**
    *   Quand l'utilisateur pose une question, on la transforme aussi en vecteurs.
    *   On cherche dans la base les morceaux qui "ressemblent" le plus √† la question (recherche de similarit√© s√©mantique).

3.  **Generation (R√©ponse)**
    *   On envoie au LLM : La question + Les morceaux trouv√©s.
    *   Prompt : *"Utilise ces morceaux pour r√©pondre √† la question"*.

## üíª Exemple de code (`4_rag_basics.py`)

```python
# 1. Charger et D√©couper
loader = TextLoader("mon_document.txt")
chunks = CharacterTextSplitter(chunk_size=500).split_documents(loader.load())

# 2. Indexer (Vector Store)
vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())
retriever = vectorstore.as_retriever()

# 3. Cha√Æne RAG
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```

## ‚ö†Ô∏è Points d'attention
- **La qualit√© des donn√©es** : "Garbage in, garbage out". Si votre document est mal √©crit ou mal d√©coup√©, la r√©ponse sera mauvaise.
- **La taille des chunks** : Trop petits, on perd le contexte. Trop grands, on noie l'info pr√©cise.
