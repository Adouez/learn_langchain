# ðŸ§  LeÃ§on 4 : La MÃ©moire (Memory)

## ðŸ“– Introduction

Par dÃ©faut, un LLM est "**stateless**" (sans Ã©tat) : il oublie tout aprÃ¨s chaque rÃ©ponse. 

Pour crÃ©er un vrai Chatbot qui se souvient de la conversation, il faut **injecter l'historique** dans le Prompt Ã  chaque tour.

---

## ðŸ”„ Comment Ã§a marche ?

Au lieu d'envoyer juste la question actuelle, on envoie :

```
[Historique des messages] + [Nouvelle Question]
```

> ðŸ’¡ **MÃ©taphore** : C'est comme un GPS avec historique. Au lieu de demander "OÃ¹ aller ?", tu demandes "En partant de lÃ  oÃ¹ j'Ã©tais, oÃ¹ aller maintenant ?".

---

## ðŸ§© Composants ClÃ©s

### 1. `MessagesPlaceholder`
Une case vide dans le Prompt Template rÃ©servÃ©e pour insÃ©rer l'historique de conversation.

```python
MessagesPlaceholder(variable_name="history")
```

C'est comme rÃ©server une place dans un formulaire pour "coller" l'historique plus tard.

### 2. `RunnableWithMessageHistory`
Un outil qui gÃ¨re automatiquement :
- La sauvegarde des messages (ce que tu dis ET ce que le bot rÃ©pond)
- La rÃ©injection de l'historique au tour suivant

### 3. `session_id`
Un identifiant unique pour distinguer les conversations de diffÃ©rents utilisateurs.

> ðŸ’¡ Alice et Bob peuvent parler au mÃªme bot sans mÃ©langer leurs historiques grÃ¢ce au `session_id`.

---

## ðŸ’» Exemple de Code

**Fichier** : `scripts/3_memory.py`

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# 1. Prompt avec emplacement pour l'historique
prompt = ChatPromptTemplate.from_messages([
    ("system", "Tu es un assistant amical."),
    MessagesPlaceholder(variable_name="history"),  # <-- La mÃ©moire
    ("human", "{question}")
])

# 2. Stockage de l'historique (en RAM ici)
store = {}

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# 3. ChaÃ®ne avec mÃ©moire
chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="question",
    history_messages_key="history"
)

# 4. Utilisation avec session_id
response = chain_with_history.invoke(
    {"question": "Je m'appelle Alice"},
    config={"configurable": {"session_id": "user_123"}}
)
```

---

## ðŸ“Š Flux de la MÃ©moire

```
Tour 1: "Je m'appelle Alice"
        â†“
[history: vide] + "Je m'appelle Alice" â†’ Bot: "Bonjour Alice !"
        â†“
Sauvegarde: [Human: "Je m'appelle Alice", AI: "Bonjour Alice !"]

Tour 2: "Quel est mon nom ?"
        â†“
[history: Human+AI prÃ©cÃ©dents] + "Quel est mon nom ?" â†’ Bot: "Tu es Alice !"
```

---

## âœ… Points Ã  Retenir

- Les LLMs sont **stateless** par dÃ©faut (sans mÃ©moire)
- `MessagesPlaceholder` = L'emplacement rÃ©servÃ© pour l'historique
- `RunnableWithMessageHistory` = Le gestionnaire automatique de mÃ©moire
- `session_id` = Identifiant unique par utilisateur/conversation

---

## ðŸ”œ Prochaine LeÃ§on

Notre bot a de la mÃ©moire, mais il ne connaÃ®t que ce qu'il a appris pendant son entraÃ®nement. Comment lui faire lire VOS documents ? C'est le **RAG** !
