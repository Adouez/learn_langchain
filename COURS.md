# ü¶ú Cours LangChain : De Z√©ro √† H√©ros

Ce document regroupe les le√ßons progressives pour ma√Ætriser LangChain.

## üéì Le√ßon 1 : Les Fondations

LangChain est un framework qui permet de connecter des mod√®les de langage (LLMs) √† d'autres sources de donn√©es et d'outils.

Si GPT-4 est le **moteur**, LangChain est le **ch√¢ssis** qui permet de construire une voiture autour.

### Concepts Cl√©s :
- **LLM (Large Language Model)** : Le "cerveau" (ex: GPT-3.5, GPT-4).
- **API Key** : Le pass d'acc√®s pour utiliser le cerveau.

### Exemple de code (`1_hello_langchain.py`) :
Une connexion simple pour poser une question brute.
```python
llm = ChatOpenAI(api_key="...", model="gpt-3.5-turbo")
response = llm.invoke("Bonjour !")
```

---

## üß© Le√ßon 2 : Structure et Prompt Templates

Les LLMs sont sensibles √† la fa√ßon dont on leur parle. Pour obtenir de bons r√©sultats de mani√®re constante, on ne pose pas des questions brutes, on utilise des **Prompts Templates**.

### Pourquoi utiliser des Templates ?
1. **R√©utilisabilit√©** : On cr√©e un "moule" et on change juste les variables.
2. **Contextualisation** : On peut donner un "R√¥le" au mod√®le (System Message) pour qu'il agisse comme un expert.

### Concepts Cl√©s :
- **System Message** : Instruction cach√©e qui d√©finit le comportement du bot ("Tu es un expert...").
- **Human Message** : La question de l'utilisateur.
- **Variable (`{question}`)** : La partie dynamique du template.

---

## üîó Le√ßon 3 : Les Cha√Ænes (Chains) et LCEL

C'est le c≈ìur de LangChain. Une "Chain" est une s√©quence d'actions.

### La syntaxe LCEL (LangChain Expression Language)
LangChain utilise une syntaxe √©l√©gante avec le symbole pipe `|` (comme en Unix/PowerShell) pour encha√Æner les composants.

```python
chain = prompt | llm | output_parser
```
**Lecture :** "Prends l'entr√©e, passe-la au **Prompt**, envoie le r√©sultat au **LLM**, puis nettoie la sortie avec le **Parser**."

### Les Composants de la Cha√Æne :
1. **Prompt** : Formate la question.
2. **LLM** : G√©n√®re la r√©ponse brute (objet Message).
3. **Output Parser** : Transforme l'objet Message en texte simple (string).

### Exemple (`2_chains.py`) :
```python
prompt = ChatPromptTemplate.from_messages(...)
chain = prompt | llm | StrOutputParser()
chain.invoke({"question": "Explique..."})
```

---

## üß† Le√ßon 4 : La M√©moire (Memory)

Par d√©faut, un LLM est "stateless" (sans √©tat) : il oublie tout apr√®s chaque r√©ponse. Pour cr√©er un Chatbot, il faut injecter l'historique de la conversation dans le Prompt √† chaque tour.

### Comment √ßa marche ?
Au lieu d'envoyer juste la question actuelle, on envoie :
`[Historique des messages] + [Nouvelle Question]`

### Composants Cl√©s :
1. **`MessagesPlaceholder`** : Une case vide dans le Prompt Template r√©serv√©e pour ins√©rer l'historique.
2. **`RunnableWithMessageHistory`** : Un outil qui g√®re automatiquement la sauvegarde des messages (ce que tu dis et ce que le bot r√©pond) et leur r√©injection au tour suivant.
3. **`session_id`** : Permet de distinguer les conversations de diff√©rents utilisateurs (ex: Alice vs Bob).

### Exemple (`3_memory.py`) :
```python
# Dans le prompt
MessagesPlaceholder(variable_name="history")

# L'ex√©cution avec session_id
chain_with_history.invoke(
    {"question": "..."},
    config={"configurable": {"session_id": "user_123"}}
)
```

---

## üîú √Ä venir : Le√ßon 5 - Le RAG (Retrieval Augmented Generation)

Maintenant que notre bot a de la m√©moire, comment lui donner acc√®s √† VOS propres donn√©es (fichiers PDF, texte...) qu'il ne connait pas ? C'est le RAG.

